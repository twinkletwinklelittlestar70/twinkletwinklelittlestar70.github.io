<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> An idea to extract fake parts of fake news · littlestar</title><meta name="description" content="An idea to extract fake parts of fake news - Lin"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="../../../../favicon.png"><link rel="stylesheet" href="../../../../css/hermes.css"><link rel="search" type="application/opensearchdescription+xml" href="https://twinkletwinklelittlestar70.github.io/atom.xml" title="littlestar"><meta name="generator" content="Hexo 5.3.0"><link rel="alternate" href="atom.xml" title="littlestar" type="application/atom+xml">
</head><body><div class="wrap"><header><a class="logo-link" href="../../../../index.html"><img src="../../../../favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a class="nav-list-link" href="../../../../index.html" target="_self">BLOG</a></li><li class="nav-list-item"><a class="nav-list-link" href="../../../../https:/github.com/twinkletwinklelittlestar70" target="_blank">GITHUB</a></li><li class="nav-list-item"><a class="nav-list-link" href="../../../../archives/" target="_self">ARCHIVE</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">An idea to extract fake parts of fake news</h1><div class="post-info">May 12, 2022</div><div class="post-content"><p>This problem occurred when we were trying to improve a Chatbot user experience. This chatbot is a coursework that supports covid-related Q&amp;A, fake tweet recognition, and rapid diagnosis of covid by cough. For more details refer to the <a target="_blank" rel="noopener" href="https://github.com/twinkletwinklelittlestar70/CovBot">project repo</a>.</p>
<p>When doing the practice module of NLP course, we intend to do a recognition for fake news on Twitter about Covid-19. The teacher then suggested that instead of just identifying fake news, we could try to figure out which parts of a tweet hold the fake facts.</p>
<p>We have fine-tuned a BERT model and obtained its attention output. These steps are easy and there is a lot of information on the web. But how to infer which parts of the original text are fake and which parts are real from the attention data is a very specific problem, I can’t find any similar solution on the Internet. And there is even no available code example for attention visualization of any text classification problem.</p>
<p>So after reading various materials and combining bert-viz and my own understanding, this idea occurs to me to extract the most important part of a BERT model classification result and it performs not bad. Now let me start to introduce.</p>
<h2 id="Idea-of-extracting-fake-part-by-attention-visualization"><a href="#Idea-of-extracting-fake-part-by-attention-visualization" class="headerlink" title="Idea of extracting fake part by attention visualization"></a>Idea of extracting fake part by attention visualization</h2><p>The most basic idea is that we believe that the fake parts of a tweet is the parts that has the greatest impact on the fake recognition result, that is, the parts with the highest weight in the model attention output. So our problem becomes, how to find the parts with the largest weight from the attention.</p>
<h2 id="Understand-the-attention-output"><a href="#Understand-the-attention-output" class="headerlink" title="Understand the attention output"></a>Understand the attention output</h2><p>放huge face的transformer 文档，解释transformer的attention输出的结构。</p>
<p>然后参考bert-viz这个可视化，理解可视化是在做什么</p>
<p>To find the parts with the largest weight, we need to understand the data structure of attention output first. The pre-trained BERT we use is from huggingface and here is the <a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForSequenceClassification">document of the classifier</a>.</p>
<p>According to their document, the output attention has 4 dimentions which represent batch_size, num_heads, sequence_length and sequence_length separately.<br><img src="../../../../images/attention2.png"/></p>
<p>Now that we have the data, how do we visualize it? One of the most common, and the simplest, is grayscale. A typical grayscale image visualization attention is as follows.<br><img src="../../../../images/attention3.png" style="width: 60%;" /></p>
<p>Obviously this kind of attention results that can only show one layer. Of course, we can also do statistics to each layer, get a final result, and then finally display it. Another common way to do the visualization is implemented by bert-viz.<br><img src="../../../../images/attention4.png"/></p>
<p>But all these methods to do the visualization is very user unfriendly obviously. As a function of a chatbot, we hope that after the user enters text, we can directly give the result of whether it is fake news, and at the same time highlight the fake part.</p>
<h2 id="Pseudocode"><a href="#Pseudocode" class="headerlink" title="Pseudocode"></a>Pseudocode</h2><p>Here is my final pseudocode.<br><img src="../../../../images/attention1.png"/></p>
<p>And here is what it looks like in our chatbot.<br><img src="../../../../images/attention5.png"/></p>
<h2 id="Example-code"><a href="#Example-code" class="headerlink" title="Example code"></a>Example code</h2><p>The complete code and runnable example can be accessed in <a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1r5aJI0weKEd9VqNNI7y7j5pOuuVHgP_q#scrollTo=BlYAvI05la6N">colab</a>. It should be noted that the following code only uses the first layer of attention data for convenience. If you want to count all the attention, you should add a traversal of attention layers.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> IPython.core.display <span class="keyword">import</span> display, HTML</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_attention</span>(<span class="params">input_ids_all, attentions_all, tokenizer</span>):</span></span><br><span class="line">    html = []</span><br><span class="line">    <span class="keyword">for</span> input_ids, attention <span class="keyword">in</span> <span class="built_in">zip</span>(input_ids_all, attentions_all): </span><br><span class="line">        one_html = []</span><br><span class="line">        tokens = tokenizer.convert_ids_to_tokens(input_ids)</span><br><span class="line">        first_layer = attention[<span class="number">0</span>]</span><br><span class="line">        count_dict = <span class="built_in">dict</span>()</span><br><span class="line">        <span class="keyword">for</span> token, attention_128 <span class="keyword">in</span> <span class="built_in">zip</span>(tokens, first_layer): </span><br><span class="line">          <span class="keyword">if</span> token == <span class="string">&#x27;[PAD]&#x27;</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">          attention_128 = attention_128.tolist()</span><br><span class="line">          attention_max = <span class="built_in">max</span>(attention_128)</span><br><span class="line">          attention_index = attention_128.index(attention_max)</span><br><span class="line">          candidate_token = tokens[attention_index]</span><br><span class="line">          <span class="keyword">if</span> candidate_token <span class="keyword">in</span> count_dict:</span><br><span class="line">            count_dict[candidate_token] += <span class="number">1</span></span><br><span class="line">          <span class="keyword">else</span>:</span><br><span class="line">            count_dict[candidate_token] = <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Count the times specific token is the most importance</span></span><br><span class="line">        count_sum = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> key, value <span class="keyword">in</span> count_dict.items():</span><br><span class="line">          <span class="keyword">if</span> key == <span class="string">&#x27;[CLS]&#x27;</span> <span class="keyword">or</span> key == <span class="string">&#x27;[SEP]&#x27;</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">          count_sum += value</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> tokens:</span><br><span class="line">          <span class="keyword">if</span> token == <span class="string">&#x27;[PAD]&#x27;</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">          <span class="keyword">if</span> token == <span class="string">&#x27;[CLS]&#x27;</span> <span class="keyword">or</span> token == <span class="string">&#x27;[SEP]&#x27;</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">          <span class="keyword">if</span> token <span class="keyword">in</span> count_dict:</span><br><span class="line">            weight = count_dict[token] / count_sum</span><br><span class="line">          <span class="keyword">else</span>: </span><br><span class="line">            weight = <span class="number">0</span></span><br><span class="line">          one_html.append(<span class="string">&#x27;&lt;span style=&quot;background-color: rgb(255,255,0,&#123;0&#125;)&quot;&gt;&#123;1&#125;&lt;/span&gt;&#x27;</span>.<span class="built_in">format</span>( weight * <span class="number">2</span>, token)) </span><br><span class="line">        </span><br><span class="line">        html_string = <span class="string">&quot; &quot;</span>.join(one_html)</span><br><span class="line">        html.append(html_string)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> html</span><br><span class="line"></span><br><span class="line">html_arr = print_attention(test_text[<span class="string">&#x27;input_ids&#x27;</span>], attentions, tokenizer)</span><br><span class="line"><span class="keyword">for</span> html <span class="keyword">in</span> html_arr:</span><br><span class="line">  display(HTML(html))</span><br></pre></td></tr></table></figure>
<h2 id="Evaluation-and-performance"><a href="#Evaluation-and-performance" class="headerlink" title="Evaluation and performance"></a>Evaluation and performance</h2><p>In our scenario, this visualization method is also the interpretation of the model results, which can only be evaluated by human exports. With the help of a search engine, I’m an expert. I randomly selected twenty pieces of test data, and manually read and evaluated the effects of fake parts exports.</p>
<p>I divided the results into three labels, completely correct, correct with missing or over extraction and completely incorrect. As a result, 80% of them could find the relative fake part and 30% of them do a perfect job.<br><img src="../../../../images/attention6.png"/></p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForSequenceClassification">Huggingface BertForSequenceClassification</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/jessevig/bertviz">BertViz</a></p>
</div></article></div></main><footer><div class="paginator"><a class="next" href="../../../../2021/12/20/mt-transformer/">NEXT</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'true';
var disqus_identifier = '2022/05/12/nlp-attentionvis/';
var disqus_title = 'An idea to extract fake parts of fake news';
var disqus_url = 'https://twinkletwinklelittlestar70.github.io/2022/05/12/nlp-attentionvis/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//#{theme.disqus}.disqus.com/count.js" async></script><div class="copyright"><p>© 2019 - 2022 <a href="https://twinkletwinklelittlestar70.github.io">Lin</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/claymcleod/hexo-theme-hermes" target="_blank">hexo-theme-hermes</a>. </p><p>Logo made by <a target="_blank" rel="noopener" href="https://www.flaticon.com/authors/freepik">Freepik</a> from <a target="_blank" rel="noopener" href="https://flaticon.com">www.flaticon.com</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-210509391-1"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-210509391-1');</script></body></html>